{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Writing a Convolutional Neural Network From Scratch\n",
    "What will you do when you stuck on village with blackout for 4 days and you only have pen and paper? For me, i wrote a `CNN from Scratch` on paper. Once again, high credits goes to pandemic Corona Virus, without it, i would not have been lived as farmer once more and the idea of <i>'from scratch'</i> rised.\n",
    "\n",
    "I am sorry for not using a single image here on this blog because i was low on data and this entire blog is written on markdown(sometimes latex) only so text format might seem little disturbing also.\n",
    "\n",
    "<b>If you are here, then you are encouraged to look at the below 3 blog posts(serially) of mine(most of the concept on this blog are taken from below posts):</b>\n",
    "* [Writing a Feed forward Neural Network from Scratch on Python](https://acharyaramkrishna.com.np/2020/05/31/writing-a-deep-neural-network-from-scratch-on-python/)\n",
    "    * This post gives a brief introduction to a OOP concept of making a simple Keras like ML library.\n",
    "    * A gentle introduction to the backpropagation and gradient descent from scratch.\n",
    "* [Writing top Machine Learning Optimizers from scratch on Python](https://acharyaramkrishna.com.np/2020/06/05/writing-popular-machine-learning-optimizers-from-scratch-on-python/)\n",
    "    * Gives introduction and python code to optimizers like `GradientDescent`, `ADAM`.\n",
    "* [Writing a Image Processing Codes from Scratch on Python](https://acharyaramkrishna.com.np/2020/05/31/image-processing-class-from-scratch-on-python/)\n",
    "    * This post gives a brief introduction to convolution operation and RGB to grayscale conversion from scratch.\n",
    "    * We will be using same convolution concept here on this blog.\n",
    "\n",
    "[If you are less on time then follow this repository for all the files, also see inside the folder `quark`](https://github.com/q-viper/ML-from-Basics).\n",
    "\n",
    "## 1.1 What this blog will cover?\n",
    "* Includes `Feed forward` layer\n",
    "* A gentle introduction to `Conv2d`\n",
    "* Includes `Dropout` layer\n",
    "* Includes `Pool2d` layer\n",
    "* Includes `Flatten` layer\n",
    "* Test Cases with different architectures(4 of them) on `MNIST` dataset\n",
    "* Bonus Topics\n",
    "\n",
    "Testing a model will require huge time, my system is Dell I5 with 8gb RAM and 256gb SSD. And i had tested these models on my local machine. It had taken nearly week to find the test cases and imporve the overall concepts. Sometimes, i had to sleep my laptop for saving battery power so some epoch might be seen taken 4+hours of time. And yes, i used mobile data to post this blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preliminary Concept\n",
    "* Every layer will have the common methods(doing so will ease the overhead of method calling):\n",
    "    * `set_output_shape`\n",
    "    * `apply_activation`\n",
    "        * `Conv2d` can have functions like `relu` and convolution operation happens here\n",
    "        * `FFL` will use the `activation_fn` method on linear combination of input, weights and biases.\n",
    "        * `Pool2d` will perform pooling operations like `max, min, average`\n",
    "        * `Dropout` will perform setting input to 0 randomly\n",
    "        * `Flatten` will convert feature vectures to 1d vector \n",
    "    * `backpropagate`\n",
    "        * `Conv2d` will use the delta term of next layer to find delta term and delta parameters\n",
    "        * `FFL` \n",
    "        * `Pool2d`: error is backpropagated from the index of the output of this layer\n",
    "        * `Dropout`: propagate error through non zero output units\n",
    "        * `Flatten` : propagate error of next layer to previous by reshapping to input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Steps\n",
    "* Prepare layers\n",
    "* Prepare stacking class\n",
    "* Prepare Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Layers\n",
    "\n",
    "### 3.1.1 Feedforward Layer\n",
    "I am not going to explain much more here because a previous post about [Writing a Feed forward Neural Network from Scratch on Python](https://acharyaramkrishna.com.np/2020/05/31/writing-a-deep-neural-network-from-scratch-on-python/) has explained already.\n",
    "\n",
    "<code>\n",
    "   class FFL():\n",
    "    def __init__(self, input_shape=None, neurons=1, bias=None, weights=None, activation=None, is_bias = True):\n",
    "        np.random.seed(100)\n",
    "        self.input_shape = input_shape\n",
    "        self.neurons = neurons\n",
    "        self.isbias = is_bias\n",
    "        self.name = \"\"\n",
    "        self.w = weights\n",
    "        self.b = bias\n",
    "        if input_shape != None:\n",
    "            self.output_shape = neurons                \n",
    "        if self.input_shape != None:\n",
    "            self.weights = weights if weights != None else np.random.randn(self.input_shape, neurons)\n",
    "            self.parameters = self.input_shape *  self.neurons + self.neurons if self.isbias else 0  \n",
    "        if(is_bias):\n",
    "            self.biases = bias if bias != None else np.random.randn(neurons)\n",
    "        else:\n",
    "            self.biases = 0            \n",
    "        self.out = None\n",
    "        self.input = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        activations = [\"relu\", \"sigmoid\", \"tanh\", \"softmax\"]\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0\n",
    "        self.pdelta_weights = 0\n",
    "        self.pdelta_biases = 0        \n",
    "        if activation not in activations and activation != None:\n",
    "             raise ValueError(f\"Activation function not recognised. Use one of {activations} instead.\")\n",
    "        else:\n",
    "            self.activation = activation          \n",
    "    def activation_dfn(self, r):\n",
    "        \"\"\"\n",
    "            A method of FFL to find derivative of given activation function.\n",
    "        \"\"\"     \n",
    "        if self.activation is None:\n",
    "            return np.ones(r.shape)\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        if self.activation == 'sigmoid':\n",
    "            r = self.activation_fn(r)\n",
    "            return r * (1 - r)\n",
    "        if self.activation == \"softmax\":\n",
    "            soft = self.activation_fn(r)                                \n",
    "            diag_soft = soft*(1- soft)\n",
    "            return diag_soft  \n",
    "        if self.activation == 'relu':\n",
    "            r[r < 0] = 0\n",
    "            return r\n",
    "        return r\n",
    "    def activation_fn(self, r):\n",
    "        \"\"\"\n",
    "        A method of FFL which contains the operation and defination of given activation function.\n",
    "        \"\"\"        \n",
    "        if self.activation == 'relu':\n",
    "            r[r < 0] = 0\n",
    "            return r\n",
    "        if self.activation == None or self.activation == \"linear\":\n",
    "            return r        \n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        if self.activation == 'sigmoid':    \n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        if self.activation == \"softmax\":\n",
    "            r = r - np.max(r)\n",
    "            s = np.exp(r)\n",
    "            return s / np.sum(s)        \n",
    "    def apply_activation(self, x):\n",
    "        soma = np.dot(x, self.weights) + self.biases\n",
    "        self.out = self.activation_fn(soma)        \n",
    "        return self.out\n",
    "    def set_n_input(self):\n",
    "        self.weights = self.w if self.w != None else np.random.normal(size=(self.input_shape, self.neurons))\n",
    "    def backpropagate(self, nx_layer):\n",
    "        self.error = np.dot(nx_layer.weights, nx_layer.delta)\n",
    "        self.delta = self.error * self.activation_dfn(self.out)\n",
    "        self.delta_weights += self.delta * np.atleast_2d(self.input).T\n",
    "        self.delta_biases += self.delta\n",
    "    def set_output_shape(self):\n",
    "        self.set_n_input()\n",
    "        self.output_shape = self.neurons\n",
    "        self.get_parameters()\n",
    "    def get_parameters(self):\n",
    "        self.parameters = self.input_shape *  self.neurons + self.neurons if self.isbias else 0  \n",
    "        return self.parameters \n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Conv2d Layer\n",
    "#### 3.1.2.1 Lets initialize it first.\n",
    "\n",
    "<code>\n",
    "class Conv2d():\n",
    "    def __init__(self, input_shape=None, filters=1, kernel_size = (3, 3), isbias=True, activation=None, stride=(1, 1),             padding=\"zero\", kernel=None, bias=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = filters\n",
    "        self.isbias = isbias\n",
    "        self.activation = activation\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.p = 1 if padding != None else 0\n",
    "        self.bias = bias\n",
    "        self.kernel = kernel\n",
    "        if input_shape != None:\n",
    "            self.kernel_size = (kernel_size[0], kernel_size[1], input_shape[2], filters)\n",
    "            self.output_shape = (int((input_shape[0] - kernel_size[0] + 2 * self.p) / stride[0]) + 1, \n",
    "                                int((input_shape[1] - kernel_size[1] + 2 * self.p) / stride[1]) + 1, filters)\n",
    "            self.set_variables()\n",
    "            self.out = np.zeros(self.output_shape)\n",
    "        else:\n",
    "            self.kernel_size = (kernel_size[0], kernel_size[1])         \n",
    "</code>\n",
    "\n",
    "Initializing takes:-\n",
    "* `input_shape`:- It is the input shape of this layer. It will include tuple of `(rows, cols, num_channels)`. For any non input layer, it will be default i.e. `None`.\n",
    "* `filters`:- How many of kernel or filters are we using?\n",
    "* `kernel_size`:- It is a size of convoluting tuple of matrix or filter's `(row, cols)`. Later we will create a kernel of shape `rows, cols, input_channels, num_filters`.\n",
    "* `isbias`: Boolean value for whether we will use bias or not.\n",
    "* `activaiton`: Activation function.\n",
    "* `tride`: A tuple indicating a step of convolution operation per row, column.\n",
    "* `padding`: String indicating what operation will be done on borders, available among `[zeros, same]`.\n",
    "* `kernel`: A convoluting matrix. Recommendated not to use.\n",
    "* `bias`: A array of shape `(num_filters, 1)` will be added after each convolution operation.\n",
    "\n",
    "Few important things inside this method are:-\n",
    "* The `output_shape` of any convolution layer will be:\n",
    "\\begin{equation}\n",
    "W = \\frac{(w-f+2*p)}{s} + 1\n",
    "\\end{equation}\n",
    "\n",
    "    Where, W is output width or shape and w is input width or shape.\\\n",
    "    f is filter width.\\\n",
    "    p is padding(1 if used)\\\n",
    "    s is stride width or shape\\\n",
    "* The method `set_variables()` sets all the important parameters needed for training.\n",
    "* `self.out` will be the output of this layer and `self.dout` will be the delta out.\n",
    "* `self.delta` will be the delta term of this layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2 `set_variable()` method\n",
    "\n",
    "<code>\n",
    "def set_variables(self):\n",
    "    self.weights = self.init_param(self.kernel_size)\n",
    "    self.biases = self.init_param((self.filters, 1))\n",
    "    self.parameters = np.multiply.reduce(self.kernel_size) + self.filters if self.isbias else 1\n",
    "    self.delta_weights = np.zeros(self.kernel_size)\n",
    "    self.delta_biases = np.zeros(self.biases.shape)    \n",
    "</code>\n",
    "\n",
    "* To make our optimization easier, we are naming filter as weights. \n",
    "* The method `init_param()` initializes parameter from random normal sample.\n",
    "\n",
    "<code>\n",
    "def init_param(self, size):\n",
    "    stddev = 1/np.sqrt(np.prod(size))\n",
    "    return np.random.normal(loc=0, scale=stddev, size=size)\n",
    "</code>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3 Prepare Activation Functions\n",
    "<code>def activation_fn(self, r):\n",
    "    \"\"\"\n",
    "    A method of FFL which contains the operation and defination of given activation function.\n",
    "    \"\"\"\n",
    "    if self.activation == None or self.activation == \"linear\":\n",
    "        return r   \n",
    "    if self.activation == 'tanh': #tanh\n",
    "        return np.tanh(r)\n",
    "    if self.activation == 'sigmoid':  # sigmoid\n",
    "        return 1 / (1 + np.exp(-r))\n",
    "    if self.activation == \"softmax\":# stable softmax   \n",
    "        r = r - np.max(r)\n",
    "        s = np.exp(r)\n",
    "        return s / np.sum(s)</code>\n",
    " \n",
    "##### Recall the mathematics, \n",
    "\n",
    "\\begin{equation}\n",
    "i. tanh(soma) = \\frac{1-soma}{1+soma}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "ii. linear(soma) = soma\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "iii. sigmoid(soma) = \\frac{1}{1 + exp^{(-soma)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "iv. relu(soma) = \\max(0, soma)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "v. softmax(x_j) = \\frac{exp^{(x_j)}}{\\sum_{i=1}^n{exp^{(x_i)}}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Where, soma = XW + \\theta\n",
    "\\end{equation}\n",
    "\n",
    "And `W` is weight vector of shape `(n, w)`. `X` is input vector of shape `(m, n)` and `ùúÉ` is bias term of shape `w, 1`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.4 Prepare derivative of Activation Function\n",
    "<code>\n",
    "def activation_dfn(self, r):\n",
    "        \"\"\"\n",
    "            A method of FFL to find derivative of given activation function.\n",
    "        \"\"\"\n",
    "        if self.activation is None:\n",
    "            return np.ones(r.shape)\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        if self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "        if self.activtion == 'softmax':\n",
    "            soft = self.activation_fn(r)\n",
    "            return soft * (1 - soft)\n",
    "        if self.activation == 'relu':\n",
    "            r[r<0] = 0\n",
    "            return r\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets revise bit of calculus. \n",
    "\n",
    "##### Why do we need derivative? \n",
    "Well, if you are here then you already know that gradient descent is based upon the derivatives(gradients) of activation functions and errors. So we need to perform this derivative. But you are on your own to perform calculation. I will also explain the gradient descent later. \n",
    "\n",
    "\\begin{equation}\n",
    "i. \\frac{d(linear(x))}{d(x)} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "ii. \\frac{d(sigmoid(x))}{d(x)} = sigmoid(x)(1- sigmoid(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "iii. \\frac{d(tanh(x))}{d(x)} = \\frac{2x}{(1+x)^2} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "iv. \\frac{d(relu(x))}{d(x)} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "v. \\frac{d(softmax(x_j))}{d(x_k)} = softmax(x_j)(1- softmax(x_j)) \\space when \\space j = k \\space else\\\n",
    "\\space -softmax({x_j}).softmax({x_k})\n",
    "\\end{equation}\n",
    "\n",
    "For the sake of simplicity, we use the case of `j = k` for softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.5 Prepare a method to do feedforward on this layer\n",
    "\n",
    "<code>\n",
    "    def apply_activation(self, image):\n",
    "        for f in range(self.filters):\n",
    "            image = self.input\n",
    "            kshape = self.kernel_size\n",
    "            if kshape[0] % 2 != 1 or kshape[1] % 2 != 1:\n",
    "                raise ValueError(\"Please provide odd length of 2d kernel.\")\n",
    "            if type(self.stride) == int:\n",
    "                     stride = (stride, stride)\n",
    "            else:\n",
    "                stride = self.stride\n",
    "            shape = image.shape\n",
    "            if self.padding == \"zero\":\n",
    "                zeros_h = np.zeros((shape[1], shape[2])).reshape(-1, shape[1], shape[2])\n",
    "                zeros_v = np.zeros((shape[0]+2, shape[2])).reshape(shape[0]+2, -1, shape[2])\n",
    "                padded_img = np.vstack((zeros_h, image, zeros_h)) # add rows\n",
    "                padded_img = np.hstack((zeros_v, padded_img, zeros_v)) # add cols\n",
    "                image = padded_img\n",
    "                shape = image.shape\n",
    "            elif self.padding == \"same\":\n",
    "                h1 = image[0].reshape(-1, shape[1], shape[2])\n",
    "                h2 = image[-1].reshape(-1, shape[1], shape[2])\n",
    "                padded_img = np.vstack((h1, image, h2)) # add rows\n",
    "                v1 = padded_img[:, 0].reshape(padded_img.shape[0], -1, shape[2])\n",
    "                v2 = padded_img[:, -1].reshape(padded_img.shape[0], -1, shape[2])\n",
    "                padded_img = np.hstack((v1, padded_img, v2)) # add cols\n",
    "                image = padded_img\n",
    "                shape = image.shape\n",
    "            elif self.padding == None:\n",
    "                pass\n",
    "            rv = 0\n",
    "            cimg = []\n",
    "            for r in range(kshape[0], shape[0]+1, stride[0]):\n",
    "                cv = 0\n",
    "                for c in range(kshape[1], shape[1]+1, stride[1]):\n",
    "                    chunk = image[rv:r, cv:c]\n",
    "                    soma = (np.multiply(chunk, self.weights[:, :, :, f]))\n",
    "                    summa = soma.sum()+self.biases[f]\n",
    "                    cimg.append(summa)\n",
    "                    cv+=stride[1]\n",
    "                rv+=stride[0]\n",
    "            cimg = np.array(cimg).reshape(int(rv/stride[0]), int(cv/stride[1]))\n",
    "            self.out[:, :, f] = cimg\n",
    "        self.out = self.activation_fn(self.out)\n",
    "        return self.out\n",
    "</code>\n",
    "\n",
    "I have linked a post about convolution operation on the top of this blog. Only important part here are:-\n",
    "* For each filter \n",
    "    * do elementwise matrix multiplication and sum them all(of each channels also)\n",
    "    * Then add bias term\n",
    "    * Output of this filter will have channel(not a real color channel) of `num_filters`\n",
    "* Finally apply activation function on this output.\n",
    "\n",
    "It is clear that, if a layer have 5 filters then the output of this layer will have 5 channels also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.6 Prepare Method for Backpropagation\n",
    "\n",
    "<code>\n",
    "    def backpropagate(self, nx_layer):\n",
    "        layer = self\n",
    "        layer.delta = np.zeros((layer.input_shape[0], layer.input_shape[1], layer.input_shape[2]))\n",
    "        image = layer.input\n",
    "        for f in range(layer.filters):\n",
    "            kshape = layer.kernel_size\n",
    "            shape = layer.input_shape\n",
    "            stride = layer.stride\n",
    "            rv = 0\n",
    "            i = 0\n",
    "            for r in range(kshape[0], shape[0]+1, stride[0]):\n",
    "                cv = 0\n",
    "                j = 0\n",
    "                for c in range(kshape[1], shape[1]+1, stride[1]):\n",
    "                    chunk = image[rv:r, cv:c]\n",
    "                    layer.delta_weights[:, :, :, f] += chunk * nx_layer.delta[i, j, f]\n",
    "                    layer.delta[rv:r, cv:c, :] += nx_layer.delta[i, j, f] * layer.weights[:, :, :, f]\n",
    "                    j+=1\n",
    "                    cv+=stride[1]\n",
    "                rv+=stride[0]\n",
    "                i+=1\n",
    "            layer.delta_biases[f] = np.sum(nx_layer.delta[:, :, f])\n",
    "        layer.delta = layer.activation_dfn(layer.delta)\n",
    "</code>\n",
    "    \n",
    "Backpropagating error from Convolution layer is really hard and challenging task. I have tried my best to do right way of backpropagation but i still have doubt about it. Some really awesome articles like below can help to understand these things:-\n",
    "* [Convolutional Neural Network from Ground Up](https://towardsdatascience.com/convolutional-neural-network-from-ground-up-c67bb41454e1)\n",
    "* [A Gentle Introduction to CNN](https://sefiks.com/2017/11/03/a-gentle-introduction-to-convolutional-neural-networks/)\n",
    "* [Training a Convolutional Neural Network](https://victorzhou.com/blog/intro-to-cnns-part-2/)\n",
    "\n",
    "For understanding how to pass errors and find the delta terms for parameters:\n",
    "* The delta term for this layer will be equal to the shape of input i.e. `(input_row, input_cols, input_channels)`. \n",
    "* We will also take the input to this layer into consideration.\n",
    "* For each filters:-\n",
    "    * Loop through each row and col just like convolution operation\n",
    "    * Get the chunk or part of image and multiply it with the delta term of next layer to get delta filter(weight)\n",
    "        * i.e. `layer.delta_weights[:, :, :, f] += chunk * nx_layer.delta[i, j, f]` a trick to understand the delta of next layer is by revisiting the input and output shape of layer. For a layer with 5 filters, output will have 5 channels. And the delta term of next layer will have same number of channels. Hence we are giving `[i, j, f]`. Note that for every step on input image(i.e step on row and col), `i`, `j` will increase by 1. Initially, `layer.delta_weights[:, :, :, f]` will be all 0s but it will change by visiting every chunks. Since we have filter of shape `(row, col, channels, num_filters)`, delta_weights is updated for each filter by adding it with multiplication of each chunk with corresponding next layer's delta.\n",
    "        * Delta term of this layer will have shape of `(input_rows, input_cols, channels)` i.e equal to input shape. Hence we will set the delta term using the number of channels on this layer's filters. We will add the delta term for that chunk using each filters. Because each filters are responsible for the error and the contribution of each filter must be taken equally. The `layer.delta[rv:r, cv:c, :] += nx_layer.delta[i, j, f] * layer.weights[:, :, :, f]` is here to do this task.\n",
    "        * We increase i after completing row and j after completing column. `i` and `j` are used to get values from delta of next layer.\n",
    "    * We sum the delta term of this filter to get `delta_biases` due to this filter.\n",
    "* Finally, we get delta of this layer by applying derivative of activation function of this layer.\n",
    "\n",
    "> <b>There are different approaches than this one of doing backpropagation on Convolution layer. I found this one to be working on my case(i wrote this approach). Please try to visit one of above links for more explanation.</b>\n",
    "\n",
    "<b>Please test your class like below:-</b>\n",
    "<code>\n",
    "img = xt[0]\n",
    "conv = Conv2d()\n",
    "conv.input=img\n",
    "conv.weights = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).reshape(3, 3, 1, 1)\n",
    "conv.biases = np.zeros(1)\n",
    "conv.out = np.zeros((28, 28, 1))\n",
    "cout = conv.apply_activation(img)\n",
    "plt.imshow(cout.reshape(28, 28)) \n",
    "</code>\n",
    "\n",
    "Where `xt` is an image array of shape `(28, 28, 1)` from `mnist`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Dropout Layer\n",
    "The main concept behind the dropout layer is to forget some of the inputs to current layer forcefully. Doing so will reduce the risk of overfitting the model.\n",
    "\n",
    "<code>\n",
    "class Dropout:\n",
    "    def __init__(self, prob = 0.5):\n",
    "        self.input_shape=None\n",
    "        self.output_shape = None\n",
    "        self.input_data= None\n",
    "        self.output = None\n",
    "        self.isbias = False\n",
    "        self.activation = None\n",
    "        self.parameters = 0\n",
    "        self.delta = 0\n",
    "        self.weights = 0\n",
    "        self.bias = 0\n",
    "        self.prob = prob\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0       \n",
    "    def set_output_shape(self):\n",
    "        self.output_shape = self.input_shape\n",
    "        self.weights = 0\n",
    "    def apply_activation(self, x, train=True):\n",
    "        if train:\n",
    "            self.input_data = x\n",
    "            flat = np.array(self.input_data).flatten()\n",
    "            random_indices = np.random.randint(0, len(flat), int(self.prob * len(flat)))\n",
    "            flat[random_indices] = 0\n",
    "            self.output = flat.reshape(x.shape)\n",
    "            return self.output\n",
    "        else:\n",
    "            self.input_data = x\n",
    "            self.output = x / self.prob\n",
    "            return self.output\n",
    "    def activation_dfn(self, x):\n",
    "        return x\n",
    "    def backpropagate(self, nx_layer):\n",
    "        if type(nx_layer).__name__ != \"Conv2d\":\n",
    "            self.error = np.dot(nx_layer.weights, nx_layer.delta)\n",
    "            self.delta = self.error * self.activation_dfn(self.out)\n",
    "        else:\n",
    "            self.delta = nx_layer.delta\n",
    "        self.delta[self.output == 0] = 0\n",
    "</code>\n",
    "\n",
    "* Some of parameters like `weights`, `biases` are actually not available on the Dropout layer but i am using this for the sake of simplicity while working with stack of layers.\n",
    "* The input shape and output shape of Dropout layer will be same, what differs is the value. Where some will be set to 0 i.e forgotten randomly.\n",
    "* The method `apply_activation` performs the dropout operation. \n",
    "    * The easier way is to first convert it to 1d vector(by numpy's `flatten`) and take random indices from given probability. \n",
    "    * Then we set the element of those random indices to 0 and return the reshaped new array as output of this layer.\n",
    "* The method `backpropagate` performs the backpropagation operation on error.\n",
    "     * We set delta to `0` if the recent output of this layer is 0, else leave as it is.\n",
    "* Note:- In testing phase, forward propagation will be different. Entire activation is reduced by factor. So we are also giving a train parameter to `apply_activation`.\n",
    "     \n",
    "<b> Lets test our class:-</b>\n",
    "\n",
    "<code>\n",
    "x = np.arange(0, 100).reshape(10, 10)\n",
    "dp = Dropout()\n",
    "dp.apply_activation(x)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Pooling Layer\n",
    "A convolutional neural network's work can be thought as:\n",
    "* Take a image where we want to perform a convolution.\n",
    "* Take a lens(will be filter) and place it over an image.\n",
    "* Slide the lens over a image and find the important features.\n",
    "* We find features using different lenses.\n",
    "* Once we found certain features under our boundary, we pass those feature maps to next scanning place or we can do pooling.\n",
    "* Pooling can be thought of as zooming out, or we make the remaining image little smaller, by this way more important features will be seen. Or in other way, scan from bit far and take only important part.\n",
    "\n",
    "A pooling operation works on similar way like convolution but instead of matrix multiplication we do different operation. The output of a pooling layer will be:-\n",
    "\n",
    "\\begin{equation}\n",
    "w = \\frac{W-f + 2p}{s} + 1\n",
    "\\end{equation}\n",
    "\n",
    "where `w` is new width, `W` is old or input width, `f` is kernel width, `p` is padding. <b>I am not using padding right now for the operation.</b>\n",
    "\n",
    "\n",
    "\n",
    "#### 3.1.4.1 Initializing a Class\n",
    "\n",
    "<code>\n",
    "class Pool2d:\n",
    "    def __init__(self, kernel_size = (2, 2), stride=None, kind=\"max\", padding=None):\n",
    "        self.input_shape=None\n",
    "        self.output_shape = None\n",
    "        self.input_data= None\n",
    "        self.output = None\n",
    "        self.isbias = False\n",
    "        self.activation = None\n",
    "        self.parameters = 0\n",
    "        self.delta = 0\n",
    "        self.weights = 0\n",
    "        self.bias = 0\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0\n",
    "        self.padding = padding\n",
    "        self.p = 1 if padding != None else 0\n",
    "        self.kernel_size = kernel_size\n",
    "        if type(stride) == int:\n",
    "                 stride = (stride, stride)\n",
    "        self.stride = stride\n",
    "        if self.stride == None:\n",
    "            self.stride = self.kernel_size\n",
    "        self.pools = ['max', \"average\", 'min']\n",
    "        if kind not in self.pools:\n",
    "            raise ValueError(\"Pool kind not understoood.\")            \n",
    "        self.kind = kind\n",
    "</code>\n",
    "\n",
    "Most of attributes are common to the `Convolution layer`.\n",
    "* Just like Keras, we will set the `stride` to `kernel_size` if nothing is given.\n",
    "* The pools is a list of available pooling type. Currently, i have only included 3.\n",
    "\n",
    "#### 3.1.4.2 Method `set_output_shape`\n",
    "As always, this method will always be called from the stackking class.\n",
    "\n",
    "<code>\n",
    "    def set_output_shape(self):\n",
    "        self.output_shape = (int((self.input_shape[0] - self.kernel_size[0] + 2 * self.p) / self.stride[0] + 1), \n",
    "                            int((self.input_shape[1] - self.kernel_size[1] + 2 * self.p) / self.stride[1] + 1),                                         self.input_shape[2])       \n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4.3 Feedforward or `apply_activation` method\n",
    "This method will perform the real pooling operation indicated above.\n",
    "\n",
    "<code>\n",
    "    def apply_activation(self, image):\n",
    "        stride = self.stride\n",
    "        kshape = self.kernel_size\n",
    "        shape = image.shape\n",
    "        self.input_shape = shape\n",
    "        self.set_output_shape()\n",
    "        self.out = np.zeros((self.output_shape))\n",
    "        for nc in range(shape[2]):\n",
    "            cimg = []\n",
    "            rv = 0\n",
    "            for r in range(kshape[0], shape[0]+1, stride[0]):\n",
    "                cv = 0\n",
    "                for c in range(kshape[1], shape[1]+1, stride[1]):\n",
    "                    chunk = image[rv:r, cv:c, nc]\n",
    "                    if len(chunk) > 0:                        \n",
    "                        if self.kind == \"max\":\n",
    "                            chunk = np.max(chunk)\n",
    "                        if self.kind == \"min\":\n",
    "                            chunk = np.min(chunk)\n",
    "                        if self.kind == \"average\":\n",
    "                            chunk = np.mean(chunk)\n",
    "                        cimg.append(chunk)\n",
    "                    else:\n",
    "                        cv-=cstep\n",
    "                    cv+=stride[1]\n",
    "                rv+=stride[0]\n",
    "            cimg = np.array(cimg).reshape(int(rv/stride[0]), int(cv/stride[1]))\n",
    "            self.out[:,:,nc] = cimg\n",
    "        return self.out\n",
    "</code>\n",
    "\n",
    "Lets take an example:-\n",
    "\n",
    "\\begin{equation}\n",
    "x = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 1 \\\\ \n",
    "11 & 12 & 4 & 10 \\\\\n",
    "101 & 11 & 88 & 10 \\\\\n",
    "10 & 11 & 11 & 5 \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "After maxpool of size `(2, 2)` and stride `(2, 2)`:-\n",
    "* First our pointer will be 0 for row/col i.e `curr_pointer = (0, 0)` and  window will be values of  `curr_pointer:curr_pointer+kernel_size-1`.\n",
    "* In other words, our first window will be `[[1 2] [11, 12]]`.\n",
    "* Then for maxpool, maximum value on this window is 12, so 12 is taken, if average pool then output of this window will be `6.5` i.e average of `1, 2, 11, 12`.\n",
    "* Then current pointer of row will be `prev_pointer[0]+stride[0]`\n",
    "* Now new window will be `[[3 1] [4 10]]` and maxpool will be `10`.\n",
    "* Now we have reached the end of this row, we will increase the column. Then current pointer will be `curr_pointer + (0, stride[1]-1)`. \n",
    "\n",
    "<b>Maxpooling of `ùë•`:</b>\n",
    "\\begin{pmatrix}\n",
    "12 & 10 \\\\ \n",
    "101 & 88 \\end{pmatrix}\n",
    "\n",
    "In more simpler way, we took only those values which contributes high value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4.4 Backpropagate Method\n",
    "\n",
    "<code>\n",
    "def backpropagate(self, nx_layer):\n",
    "    \"\"\"\n",
    "        Gradients are passed through index of latest output value .\n",
    "    \"\"\"\n",
    "    layer = self\n",
    "    stride = layer.stride\n",
    "    kshape = layer.kernel_size\n",
    "    image = layer.input\n",
    "    shape = image.shape\n",
    "    layer.delta = np.zeros(shape)\n",
    "    cimg = []\n",
    "    rstep = stride[0]\n",
    "    cstep = stride[1]\n",
    "    for f in range(shape[2]):\n",
    "        i = 0\n",
    "        rv = 0\n",
    "        for r in range(kshape[0], shape[0]+1, rstep):\n",
    "            cv = 0\n",
    "            j = 0\n",
    "            for c in range(kshape[1], shape[1]+1, cstep):\n",
    "                chunk = image[rv:r, cv:c, f]\n",
    "                dout = nx_layer.delta[i, j, f]\n",
    "                if layer.kind == \"max\":\n",
    "                    p = np.max(chunk)\n",
    "                    index = np.argwhere(chunk == p)[0]\n",
    "                    layer.delta[rv+index[0], cv+index[1], f] = dout\n",
    "                if layer.kind == \"min\":\n",
    "                    p = np.min(chunk)\n",
    "                    index = np.argwhere(chunk == p)[0]\n",
    "                    layer.delta[rv+index[0], cv+index[1], f] = dout\n",
    "                if layer.kind == \"average\":\n",
    "                    p = np.mean(chunk)\n",
    "                    layer.delta[rv:r, cv:c, f] = dout\n",
    "                j+=1\n",
    "                cv+=cstep\n",
    "            rv+=rstep\n",
    "            i+=1\n",
    "</code>\n",
    "\n",
    "Main idea behind the backpropagation on Pooling Layer is:-\n",
    "* If pooling is `Max` then error is passed through index of largest value on chunk.\n",
    "* If pooling is `Min`then error is passed through index of smallest value on chunk.\n",
    "* If pooling is `average` then error is passed through entire indices on chunk\n",
    "\n",
    "Since the output shape and input shape's number of channel remains same, we loop through each channel and get the delta for each channel. So we are not adding the delta term.\n",
    "\n",
    "<b>Lets test our pooling class:</b>\n",
    "\n",
    "<code>\n",
    "pool = Pool2d(kernel_size=(7, 7), kind=\"max\")\n",
    "test = np.random.randint(1, 100, (32, 32, 3))\n",
    "o = pool.apply_activation(test)\n",
    "</code>\n",
    "\n",
    "If you don't get any error then, great lets proceed. Else please see the reference file on github.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Flatten Layer\n",
    "Flatten layer's main task is to take entire feature maps of previous layer and make a 1d vector from it. Flatten layer is used before passing a result of convolution to classification layers.\n",
    "\n",
    "Let the input to `Flatten` be `(3, 3, 3)`.\n",
    "\\begin{equation}\n",
    "x = \n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "1 & 11 & 12\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "4 & 10 & 1\\end{pmatrix}\\\\\n",
    "\\begin{pmatrix}\n",
    "101 & 11 & 88\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "10 & 11 & 11\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "5 & 111 & 33\\end{pmatrix}\\\\\n",
    "\\begin{pmatrix}\n",
    "9 & 11 & 123\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "66 & 110 & 12\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "100 & 11 & 12\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Flatten output will be:\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3&\n",
    "1 & 11 & 12&\n",
    "4 & 10 & 1&\n",
    "101 & 11 & 88&\n",
    "10 & 11 & 11&\n",
    "5 & 111 & 33&\n",
    "9 & 11 & 123&\n",
    "66 & 110 & 12&\n",
    "100 & 11 & 12&\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<code>\n",
    "class Flatten:\n",
    "    def __init__(self, input_shape=None):\n",
    "        self.input_shape=None\n",
    "        self.output_shape = None\n",
    "        self.input_data= None\n",
    "        self.output = None\n",
    "        self.isbias = False\n",
    "        self.activation = None\n",
    "        self.parameters = 0\n",
    "        self.delta = 0\n",
    "        self.weights = 0\n",
    "        self.bias = 0\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0        \n",
    "    def set_output_shape(self):\n",
    "        self.output_shape = (self.input_shape[0] * self.input_shape[1] * self.input_shape[2])\n",
    "        self.weights = 0\n",
    "    def apply_activation(self, x):\n",
    "        self.input_data = x\n",
    "        self.output = np.array(self.input_data).flatten()\n",
    "        return self.output\n",
    "    def activation_dfn(self, x):\n",
    "        return x\n",
    "    def backpropagate(self, nx_layer):\n",
    "        self.error = np.dot(nx_layer.weights, nx_layer.delta)\n",
    "        self.delta = self.error * self.activation_dfn(self.out)\n",
    "        self.delta = self.delta.reshape(self.input_shape)\n",
    "</code>\n",
    "\n",
    "<b>Note: There will be no attributes like `weights`, `biases` on `Flatten` layer but i used to make it work on doing optimization</b>\n",
    "* The output shape of this layer will be the multiplication of `(num_rows, num_cols, num_channels)`.\n",
    "* Since this layer will be connected before the feedforward layer, error and delta terms are calculated like on feedforward layer.\n",
    "* The shape of delta of this layer will be shape of input.\n",
    "\n",
    "<b> Lets test our flatten class:</b>\n",
    "\n",
    "<code>\n",
    "x = np.array([[1, 1, 1], [1, 0, 1], [0, 1, 1], [0, 0, 1]])\n",
    "f = Flatten()\n",
    "print(f.apply_activation(test))  \n",
    "</code>\n",
    "\n",
    "If you got output like below, then cool:-\n",
    "\n",
    "`[1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Creating a Stackking class\n",
    "\n",
    "We will name it to `CNN`.\n",
    "\n",
    "As previous feedforward post, this will perform all the tasks like training, testing and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Initializing a class\n",
    "\n",
    "Please refer to previous post about Feedforward Neural Network for more explanation.\n",
    "\n",
    "<code>\n",
    "class CNN():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.info_df = {}\n",
    "        self.column = [\"LName\", \"Input Shape\", \"Output Shape\", \"Activation\", \"Bias\"]\n",
    "        self.parameters = []\n",
    "        self.optimizer = \"\"\n",
    "        self.loss = \"mse\"\n",
    "        self.lr = 0.01\n",
    "        self.mr = 0.0001\n",
    "        self.metrics = []\n",
    "        self.av_optimizers = [\"sgd\", \"momentum\", \"adam\"]\n",
    "        self.av_metrics = [\"mse\", \"accuracy\", \"cse\"]\n",
    "        self.av_loss = [\"mse\", \"cse\"]\n",
    "        self.iscompiled = False\n",
    "        self.model_dict = None\n",
    "        self.out = []\n",
    "        self.eps = 1e-15\n",
    "        self.train_loss = {}\n",
    "        self.val_loss = {}\n",
    "        self.train_acc = {}\n",
    "        self.val_acc = {}    \n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Creating a `add` Method\n",
    "\n",
    "Please refer to the previous post for more explanation.\n",
    "\n",
    "<code>\n",
    "def add(self, layer):\n",
    "        if(len(self.layers) > 0):\n",
    "            prev_layer = self.layers[-1]\n",
    "            if prev_layer.name != \"Input Layer\":\n",
    "                prev_layer.name = f\"{type(prev_layer).__name__}{len(self.layers) - 1}\"             \n",
    "            if layer.input_shape == None:\n",
    "                if type(layer).__name__ == \"Flatten\":\n",
    "                        ops = prev_layer.output_shape[:]\n",
    "                        if type(prev_layer).__name__ == \"Pool2d\":\n",
    "                            ops = prev_layer.output_shape[:]\n",
    "                elif type(layer).__name__ == \"Conv2d\":\n",
    "                    ops = prev_layer.output_shape[:]\n",
    "                    if type(prev_layer).__name__ == \"Pool2d\":\n",
    "                        ops = prev_layer.output_shape\n",
    "                elif type(layer).__name__ == \"Pool2d\":\n",
    "                    ops = prev_layer.output_shape[:]\n",
    "                    if type(prev_layer).__name__ == \"Pool2d\":\n",
    "                        ops = prev_layer.output_shape[:]\n",
    "                else:\n",
    "                    ops = prev_layer.output_shape\n",
    "                layer.input_shape = ops\n",
    "                layer.set_output_shape()\n",
    "            layer.name = f\"Out Layer({type(layer).__name__})\"\n",
    "        else:\n",
    "            layer.name = \"Input Layer\"\n",
    "        if type(layer).__name__ == \"Conv2d\":\n",
    "            if(layer.output_shape[0] <= 0 or layer.output_shape[1] <= 0):\n",
    "                raise ValueError(f\"The output shape became invalid [i.e. {layer.output_shape}]. Reduce filter size or increase image size.\")\n",
    "        self.layers.append(layer)\n",
    "        self.parameters.append(layer.parameters)\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Writing a `summary` method:\n",
    "Please refer to previous post for more explanation.\n",
    "\n",
    "<code>\n",
    "    def summary(self):\n",
    "        lname = []\n",
    "        linput = []\n",
    "        loutput = []\n",
    "        lactivation = []\n",
    "        lisbias = []\n",
    "        lparam = []\n",
    "        for layer in self.layers:\n",
    "            lname.append(layer.name)\n",
    "            linput.append(layer.input_shape)\n",
    "            loutput.append(layer.output_shape)\n",
    "            lactivation.append(layer.activation)\n",
    "            lisbias.append(layer.isbias)\n",
    "            lparam.append(layer.parameters)\n",
    "        model_dict = {\"Layer Name\": lname, \"Input\": linput, \"Output Shape\": loutput,\n",
    "                      \"Activation\": lactivation, \"Bias\": lisbias, \"Parameters\": lparam}    \n",
    "        model_df = pd.DataFrame(model_dict).set_index(\"Layer Name\")\n",
    "        print(model_df)\n",
    "        print(f\"Total Parameters: {sum(lparam)}\")\n",
    "</code>\n",
    "\n",
    "<b> Test the class:</b>\n",
    "\n",
    "<code>\n",
    "m = CNN()\n",
    "m.add(Conv2d(input_shape = (28, 28, 1), filters = 2, padding=None, kernel_size=(3, 3), activation=\"relu\"))\n",
    "m.add(Conv2d(filters=4, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "m.add(Pool2d(kernel_size=(2, 2)))\n",
    "m.add(Conv2d(filters=6, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "m.add(Conv2d(filters=8, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "m.add(Pool2d(kernel_size=(2, 2)))\n",
    "m.add(Dropout(0.1))\n",
    "m.add(Flatten())\n",
    "m.summary()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T12:38:51.850077Z",
     "start_time": "2020-06-01T12:38:51.841082Z"
    }
   },
   "source": [
    "### 3.2.4 Writing a `train` method\n",
    "This method is identical to the `train` method of Feed Forward Neural Network. Please refer to the previous post.\n",
    "\n",
    "<code>\n",
    "def train(self, X, Y, epochs, show_every=1, batch_size = 32, shuffle=True, val_split=0.1, val_x=None, val_y=None):     \n",
    "    self.check_trainnable(X, Y)\n",
    "    self.batch_size = batch_size\n",
    "    t1 = time.time()\n",
    "    curr_ind = np.arange(0, len(X), dtype=np.int32)\n",
    "    if shuffle: \n",
    "        np.random.shuffle(curr_ind)           \n",
    "    if type(val_x) != type(None) and type(val_y) != type(None):\n",
    "        self.check_trainnable(val_x, val_y)\n",
    "        print(\"\\nValidation data found.\\n\")\n",
    "    else:\n",
    "        val_ex = int(len(X) * val_split)\n",
    "        val_exs = []\n",
    "        while len(val_exs) != val_ex:\n",
    "            rand_ind = np.random.randint(0, len(X))\n",
    "            if rand_ind not in val_exs:\n",
    "                val_exs.append(rand_ind)\n",
    "        val_ex = np.array(val_exs)\n",
    "        val_x, val_y = X[val_ex], Y[val_ex]\n",
    "        curr_ind = np.array([v for v in curr_ind if v not in val_ex])                 \n",
    "    print(f\"\\nTotal {len(X)} samples.\\nTraining samples: {len(curr_ind)} Validation samples: {len(val_x)}.\")        \n",
    "    out_activation = self.layers[-1].activation\n",
    "    batches = []\n",
    "    len_batch = int(len(curr_ind)/batch_size) \n",
    "    if len(curr_ind)%batch_size != 0:\n",
    "        len_batch+=1\n",
    "    batches = np.array_split(curr_ind, len_batch)\n",
    "    print(f\"Total {len_batch} batches, most batch has {batch_size} samples.\\n\")\n",
    "    for e in range(epochs):            \n",
    "        err = []\n",
    "        for batch in batches:\n",
    "            a = [] \n",
    "            curr_x, curr_y = X[batch], Y[batch]\n",
    "            b = 0\n",
    "            batch_loss = 0\n",
    "            for x, y in zip(curr_x, curr_y):\n",
    "                out = self.feedforward(x)\n",
    "                loss, error = self.apply_loss(y, out)\n",
    "                batch_loss += loss\n",
    "                err.append(error)\n",
    "                update = False                    \n",
    "                if b == batch_size-1:\n",
    "                    update = True\n",
    "                    loss = batch_loss/batch_size\n",
    "                self.backpropagate(loss, update)\n",
    "                b+=1\n",
    "        if e % show_every == 0:      \n",
    "            train_out = self.predict(X[curr_ind])\n",
    "            train_loss, train_error = self.apply_loss(Y[curr_ind], train_out)                \n",
    "            val_out = self.predict(val_x)\n",
    "            val_loss, val_error = self.apply_loss(val_y, val_out)                \n",
    "            if out_activation == \"softmax\":\n",
    "                train_acc = train_out.argmax(axis=1) == Y[curr_ind].argmax(axis=1)\n",
    "                val_acc = val_out.argmax(axis=1) == val_y.argmax(axis=1)\n",
    "            elif out_activation == \"sigmoid\":\n",
    "                train_acc = train_out > 0.7\n",
    "                val_acc = val_out > 0.7                    \n",
    "            elif out_activation == None:\n",
    "                train_acc = abs(Y[curr_ind]-train_out) < 0.000001\n",
    "                val_acc = abs(Y[val_ex]-val_out) < 0.000001                    \n",
    "            self.train_loss[e] = round(train_error.mean(), 4)\n",
    "            self.train_acc[e] = round(train_acc.mean() * 100, 4)                \n",
    "            self.val_loss[e] = round(val_error.mean(), 4)\n",
    "            self.val_acc[e] = round(val_acc.mean()*100, 4)\n",
    "            print(f\"Epoch: {e}:\")\n",
    "            print(f\"Time: {round(time.time() - t1, 3)}sec\")\n",
    "            print(f\"Train Loss: {round(train_error.mean(), 4)} Train Accuracy: {round(train_acc.mean() * 100, 4)}%\")\n",
    "            print(f'Val Loss: {(round(val_error.mean(), 4))} Val Accuracy: {round(val_acc.mean() * 100, 4)}% \\n')     \n",
    "            t1 = time.time()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 `check_trainnable` method\n",
    "This method does same work like previous post's method. \n",
    "\n",
    "<code>\n",
    "def check_trainnable(self, X, Y):\n",
    "        if self.iscompiled == False:\n",
    "            raise ValueError(\"Model is not compiled.\")\n",
    "        if len(X) != len(Y):\n",
    "            raise ValueError(\"Length of training input and label is not equal.\")\n",
    "        if X[0].shape != self.layers[0].input_shape:\n",
    "            layer = self.layers[0]\n",
    "            raise ValueError(f\"'{layer.name}' expects input of {layer.input_shape} while {X[0].shape[0]} is given.\")\n",
    "        if Y.shape[-1] != self.layers[-1].neurons:\n",
    "            op_layer = self.layers[-1]\n",
    "            raise ValueError(f\"'{op_layer.name}' expects input of {op_layer.neurons} while {Y.shape[-1]} is given.\")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Writing `compiling` method\n",
    "This method is identical to previous post's method.\n",
    "\n",
    "<code>\n",
    "def compile_model(self, lr=0.01, mr = 0.001, opt = \"sgd\", loss = \"mse\", metrics=['mse']):\n",
    "    if opt not in self.av_optimizers:\n",
    "        raise ValueError(f\"Optimizer is not understood, use one of {self.av_optimizers}.\")\n",
    "    for m in metrics:\n",
    "        if m not in self.av_metrics:\n",
    "            raise ValueError(f\"Metrics is not understood, use one of {self.av_metrics}.\")\n",
    "    if loss not in self.av_loss:\n",
    "        raise ValueError(f\"Loss function is not understood, use one of {self.av_loss}.\")\n",
    "    self.optimizer = opt\n",
    "    self.loss = loss\n",
    "    self.lr = lr\n",
    "    self.mr = mr\n",
    "    self.metrics = metrics\n",
    "    self.iscompiled = True\n",
    "    self.optimizer = Optimizer(layers=self.layers, name=opt, learning_rate=lr, mr=mr)\n",
    "    self.optimizer = self.optimizer.opt_dict[opt]        \n",
    "</code>   \n",
    "\n",
    "##### In order to run properly, we need to have `Optimizer` class defined. [Please see this article about it.](https://acharyaramkrishna.com.np/2020/06/05/writing-popular-machine-learning-optimizers-from-scratch-on-python/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Writing `feedforward` method\n",
    "This method is also same as previous post's method.\n",
    "\n",
    "<code>\n",
    "def feedforward(self, x, train=True):\n",
    "    if train:\n",
    "        for l in self.layers:\n",
    "            l.input = x            \n",
    "            x = np.nan_to_num(l.apply_activation(x))\n",
    "            l.out = x\n",
    "        return x\n",
    "    else:\n",
    "        for l in self.layers:\n",
    "            l.input = x \n",
    "            if type(l).__name__ == \"Dropout\":\n",
    "                x = np.nan_to_num(l.apply_activation(x, train=train))\n",
    "            else:           \n",
    "                x = np.nan_to_num(l.apply_activation(x))\n",
    "            l.out = x\n",
    "        return x\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.8 Writing `apply_loss` method\n",
    "This method is identical to previous post's method.\n",
    "\n",
    "<code>\n",
    "def apply_loss(self, y, out):\n",
    "    if self.loss == \"mse\":\n",
    "        loss = y - out\n",
    "        mse = np.mean(np.square(loss))       \n",
    "        return loss, mse\n",
    "    if self.loss == 'cse':\n",
    "        \"\"\" Requires out to be probability values. \"\"\"     \n",
    "        if len(out) == len(y) == 1: #print(\"Using Binary CSE.\")            \n",
    "            cse = -(y * np.log(out) + (1 - y) * np.log(1 - out))\n",
    "            loss = -(y / out - (1 - y) / (1 - out))\n",
    "        else: #print(\"Using Categorical CSE.\")            \n",
    "            if self.layers[-1].activation == \"softmax\":\n",
    "                \"\"\"if o/p layer's fxn is softmax then loss is y - out\n",
    "                check the derivation of softmax and crossentropy with derivative\"\"\"\n",
    "                loss = y - out\n",
    "                loss = loss / self.layers[-1].activation_dfn(out)\n",
    "            else:\n",
    "                y = np.float64(y)\n",
    "                out += self.eps\n",
    "                loss = -(np.nan_to_num(y / out) - np.nan_to_num((1 - y) / (1 - out)))\n",
    "            cse = -np.sum((y * np.nan_to_num(np.log(out)) + (1 - y) * np.nan_to_num(np.log(1 - out))))\n",
    "        return loss, cse\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9 Writing `backpropagate` method\n",
    "This method is identical to previous post's method.\n",
    "\n",
    "<code>\n",
    "    def backpropagate(self, loss, update):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:\n",
    "                if (type(layer).__name__ == \"FFL\"):\n",
    "                    layer.error = loss\n",
    "                    layer.delta = layer.error * layer.activation_dfn(layer.out)\n",
    "                    layer.delta_weights += layer.delta * np.atleast_2d(layer.input).T\n",
    "                    layer.delta_biases += layer.delta\n",
    "            else:\n",
    "                nx_layer = self.layers[i+1]\n",
    "                layer.backpropagate(nx_layer)\n",
    "            if update:\n",
    "                layer.delta_weights /= self.batch_size\n",
    "                layer.delta_biases /= self.batch_size\n",
    "        if update: \n",
    "            self.optimizer(self.layers)\n",
    "            self.zerograd()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.10`zero_grad` method \n",
    "Same as previous.\n",
    "\n",
    "<code>\n",
    " def zerograd(self):\n",
    "    for l in self.layers:\n",
    "        try:\n",
    "            l.delta_weights=np.zeros(l.delta_weights.shape)\n",
    "            l.delta_biases = np.zeros(l.delta_biases.shape)\n",
    "        except:\n",
    "            pass\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.11 `predict` method \n",
    "Same as previous.\n",
    "\n",
    "<code>\n",
    "def predict(self, X):\n",
    "    out = []\n",
    "    if X.shape != self.layers[0].input_shape:\n",
    "        for x in X:\n",
    "            out.append(self.feedforward(x, train=False))            \n",
    "    else:\n",
    "        out.append(self.feedforward(X, train = False))\n",
    "    return np.array(out)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Preparing Optimizers\n",
    "* [Please refer to this article for optimizers code.](https://acharyaramkrishna.com.np/2020/06/05/writing-popular-machine-learning-optimizers-from-scratch-on-python/) \n",
    "* [Or find these entire code on this notebook.](https://github.com/q-viper/ML-from-Basics/blob/master/Optimizers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Testing with our Model\n",
    "## 4.1 Prepare datasets\n",
    "\n",
    "<b>Note:- More the training samples, more the performance of model(but not always). But more samples takes more time to complete epoch.</b>\n",
    "\n",
    "<code>\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "x = x_train.reshape(-1, 28 * 28) \n",
    "x = (x-x.mean(axis=1).reshape(-1, 1))/x.std(axis=1).reshape(-1, 1)\n",
    "x = x.reshape(-1, 28, 28, 1) \n",
    "y = pd.get_dummies(y_train).to_numpy()\n",
    "xt = x_test.reshape(-1, 28 * 28) \n",
    "xt = (xt-xt.mean(axis=1).reshape(-1, 1))/xt.std(axis=1).reshape(-1, 1)\n",
    "xt = xt.reshape(-1, 28, 28, 1) \n",
    "yt = pd.get_dummies(y_test).to_numpy()\n",
    "</code>\n",
    "\n",
    "## 4.2 Test 1:- Model with only one `Conv2d` and `Output` layer\n",
    "\n",
    "<code>\n",
    "m = CNN()\n",
    "m.add(Conv2d(input_shape = (28, 28, 1), filters = 8, padding=None, kernel_size=(3, 3), activation=\"relu\"))\n",
    "m.add(Flatten())\n",
    "m.add(FFL(neurons = 10, activation='softmax'))\n",
    "m.compile_model(lr=0.01, opt=\"adam\", loss=\"cse\", mr=0.001)\n",
    "m.summary()\n",
    "</code>\n",
    "\n",
    "### 4.2.1 Train model\n",
    "For the sake of simplicity i am using only 1000 samples from our this test. Additionally we will use 100 of testing samples too.\n",
    "\n",
    "<code>\n",
    "m.train(x[:1000], y[:1000], epochs=100, batch_size=32, val_x=xt[:100], val_y=yt[:100])\n",
    "</code>\n",
    "\n",
    "The validation accuracy of model will not be that satisfactory but we can give it a try. \n",
    "\n",
    "After 70th epoch:\n",
    "\n",
    "<code>\n",
    "Epoch: 70, Time: 310.139sec\n",
    "Train Loss: 1707.1975 Train Accuracy: 76.7%\n",
    "Val Loss: 320.0215 Val Accuracy: 63.0% \n",
    "</code>\n",
    "\n",
    "<b>When using entire datasets, the model's performance will be great.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Test 2:- Model with 2 `Conv2d` and Output Layer\n",
    "<code>\n",
    "m.add(Conv2d(input_shape = (28, 28, 1), filters = 8, padding=None, kernel_size=(3, 3), activation=\"relu\"))\n",
    "m.add(Conv2d(filters=16, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "</code>\n",
    "\n",
    "### 4.3.1 Train model\n",
    "Lets take 10000 of training samle and 500 of validation samples. Time to perform a epoch will be huge but accuracy will be great.\n",
    "\n",
    "<code>\n",
    "m.train(x[:10000], y[:10000], epochs=100, batch_size=32, val_x=xt[:500], val_y=yt[:500])\n",
    "</code>\n",
    "\n",
    "Output is something like below:-\n",
    "\n",
    "<code>\n",
    "Epoch: 0, Time: 10528.569sec\n",
    "Train Loss: 21003.3815 Train Accuracy: 53.89%\n",
    "Val Loss: 1072.7608 Val Accuracy: 54.0% \n",
    "</code>\n",
    "<code>\n",
    "Epoch: 1, Time: 11990.521sec\n",
    "Train Loss: 16945.815 Train Accuracy: 67.44%\n",
    "Val Loss: 845.8146 Val Accuracy: 68.0% \n",
    "</code>\n",
    "<code>\n",
    "Epoch: 2, Time: 10842.482sec\n",
    "Train Loss: 14382.4224 Train Accuracy: 72.69%\n",
    "Val Loss: 790.7897 Val Accuracy: 70.2% \n",
    "</code>\n",
    "<code>\n",
    "Epoch: 3, Time: 9787.258sec\n",
    "Train Loss: 10966.7249 Train Accuracy: 80.29%\n",
    "Val Loss: 585.6976 Val Accuracy: 78.8%  \n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Epoch: 4, Time: 10025.688sec\n",
    "Train Loss: 9367.4941 Train Accuracy: 83.1%\n",
    "Val Loss: 487.3858 Val Accuracy: 81.8% \n",
    "</code>\n",
    "\n",
    "\n",
    "> It is clear that our model's performance will be good after training more with more data. To be honest, our model's performance is not as good as `keras` but it is worth trying to code it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Test 3:- A complex model\n",
    "Lets test our new model, which will have all previous assumed layers.\n",
    "<code>\n",
    "m = CNN()\n",
    "m.add(Conv2d(input_shape = (28, 28, 1), filters = 4, padding=None, kernel_size=(3, 3), activation=\"relu\"))\n",
    "m.add(Pool2d(kernel_size=(2, 2)))\n",
    "m.add(Conv2d(filters=8, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Flatten())\n",
    "m.add(FFL(neurons = 10, activation='softmax'))\n",
    "m.compile_model(lr=0.001, opt=\"adam\", loss=\"cse\")\n",
    "m.summary()\n",
    "m.train(x[:5000], y[:5000], epochs=100, batch_size=32, val_x=xt[:500], val_y=yt[:500]) \n",
    "</code>\n",
    "\n",
    "<b> Note that, since this model is huge(have many layers) the time to perform single epoch migh be huge so i am taking only `5000` of training examples and `500` of testing samples.</b>\n",
    "\n",
    "The result on my machine is:-\n",
    "\n",
    "<code>\n",
    "                      Input Output Shape Activation   Bias  Parameters\n",
    "Layer Name                                                            \n",
    "Input Layer     (28, 28, 1)  (26, 26, 4)       relu   True          40\n",
    "Pool2d1         (26, 26, 4)  (13, 13, 4)       None  False           0\n",
    "Conv2d2         (13, 13, 4)  (11, 11, 8)       relu   True         296\n",
    "Dropout3        (11, 11, 8)  (11, 11, 8)       None  False           0\n",
    "Flatten4        (11, 11, 8)          968       None  False           0\n",
    "Out Layer(FFL)          968           10    softmax   True        9690\n",
    "Total Parameters: 10026\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Total 5000 samples.\n",
    "Training samples: 5000 Validation samples: 500.\n",
    "Total 157 batches, most batch has 32 samples.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Epoch: 0:\n",
    "Time: 1640.885sec\n",
    "Train Loss: 99970.6308 Train Accuracy: 15.52%\n",
    "Val Loss: 10490.2164 Val Accuracy: 13.8% \n",
    "</code>\n",
    "The first epoch doesn't seem that much of satisfactionary but what might be other epoch?\n",
    "\n",
    "<code>\n",
    "Epoch: 10:\n",
    "Time: 1295.361sec\n",
    "Train Loss: 37848.7813 Train Accuracy: 57.68%\n",
    "Val Loss: 4674.9309 Val Accuracy: 53.4%\n",
    "</code>\n",
    "\n",
    "It is quite clear that model is progressing slowly. And 22nd epoch is:-\n",
    "\n",
    "<code>\n",
    "Epoch: 22:\n",
    "Time: 1944.176sec\n",
    "Train Loss: 22731.3455 Train Accuracy: 76.42%\n",
    "Val Loss: 3017.2488 Val Accuracy: 69.2% \n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Epoch: 35:\n",
    "Time: 1420.809sec\n",
    "Train Loss: 17295.6898 Train Accuracy: 83.1%\n",
    "Val Loss: 2358.6877 Val Accuracy: 76.2% \n",
    "</code>\n",
    "\n",
    "Similar model on `keras` gives 90+ accuracy within 5th epoch but good think about our model is, it is training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Test 4:- A complex model\n",
    "Our model doesn't seem to do great on previous complex architecture. But what if we modified it little bit? I am using my days to train these model and i have also done lots of hit and trial also.\n",
    "\n",
    "<code>\n",
    "m = CNN()\n",
    "m.add(Conv2d(input_shape = (28, 28, 1), filters = 4, padding=None, kernel_size=(3, 3), activation=\"relu\"))\n",
    "m.add(Conv2d(filters=8, kernel_size=(3, 3), padding=None, activation=\"relu\"))\n",
    "m.add(Pool2d(kernel_size=(2, 2)))\n",
    "m.add(Flatten())\n",
    "m.add(FFL(neurons = 64, activation = \"relu\"))\n",
    "m.add(Dropout(0.1))\n",
    "m.add(FFL(neurons = 10, activation='softmax'))\n",
    "m.compile_model(lr=0.01, opt=\"adam\", loss=\"cse\")\n",
    "m.summary()\n",
    "m.train(x[:10000], y[:10000], epochs=100, batch_size=32, val_x=xt[:500], val_y=yt[:500]) \n",
    "</code>\n",
    "\n",
    "The summary is:-\n",
    "\n",
    "<code>\n",
    "                       Input Output Shape Activation   Bias  Parameters\n",
    "Layer Name                                                            \n",
    "Input Layer     (28, 28, 1)  (26, 26, 4)       relu   True          40\n",
    "Conv2d1         (26, 26, 4)  (24, 24, 8)       relu   True         296\n",
    "Pool2d2         (24, 24, 8)  (12, 12, 8)       None  False           0\n",
    "Flatten3        (12, 12, 8)         1152       None  False           0\n",
    "FFL4                   1152           64       relu   True       73792\n",
    "Dropout5                 64           64       None  False           0\n",
    "Out Layer(FFL)           64           10    softmax   True         650\n",
    "Total Parameters: 74778\n",
    "</code>\n",
    "\n",
    "Model's Performance is:\n",
    "\n",
    "<code>\n",
    "Epoch: 5:\n",
    "Time: 40305.135sec\n",
    "Train Loss: 1412678.6095 Train Accuracy: 22.43%\n",
    "Val Loss: 72887.904 Val Accuracy: 24.6% \n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Epoch: 11:\n",
    "Time: 7287.762sec\n",
    "Train Loss: 512155.8547 Train Accuracy: 53.53%\n",
    "Val Loss: 28439.2441 Val Accuracy: 51.6% \n",
    "</code>\n",
    "\n",
    "<code>\n",
    "Epoch: 14:\n",
    "Time: 5984.871sec\n",
    "Train Loss: 356893.9608 Train Accuracy: 62.85%\n",
    "Val Loss: 19256.6702 Val Accuracy: 61.0% \n",
    "</code>\n",
    "\n",
    "<i>Model is progressing......</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bonus Topics\n",
    "* Good thing, these topics are interesting.\n",
    "* Bad thing, you are on your own(but you can leave a comment if explanation needed)\n",
    "## 5.1 Save Model \n",
    "This method can be placed inside the class that is stackking the layers. Else pass the model object.\n",
    "\n",
    "<code>\n",
    "def save_model(self, path=\"model.json\"):\n",
    "        \"\"\"\n",
    "            path:- where to save a model including filename\n",
    "            saves Json file on given path.\n",
    "        \"\"\"\n",
    "        dict_model = {\"model\":str(type(self).__name__)}\n",
    "        to_save = [\"name\", \"isbias\", \"neurons\", \"input_shape\", \"output_shape\", \n",
    "                   \"weights\", \"biases\", \"activation\", \"parameters\", \"filters\",\n",
    "                  \"kernel_size\", \"padding\", \"prob\", \"stride\", \"kind\"]\n",
    "        for l in self.layers:\n",
    "            current_layer = vars(l)\n",
    "            values = {\"type\":str(type(l).__name__)}\n",
    "            for key, value in current_layer.items():\n",
    "                if key in to_save:\n",
    "                    if key in [\"weights\", \"biases\"]:\n",
    "                        try:\n",
    "                            value = value.tolist()\n",
    "                        except:\n",
    "                            value = float(value)                  \n",
    "                    if type(value)== np.int32:\n",
    "                        value = float(value)\n",
    "                    if key == \"input_shape\" or key == \"output_shape\":\n",
    "                        try:\n",
    "                            value = tuple(value)\n",
    "                        except:\n",
    "                            pass\n",
    "                    values[key] = value\n",
    "            dict_model[l.name] = values\n",
    "        json_dict = json.dumps(dict_model)    \n",
    "        with open(path, mode=\"w\") as f:\n",
    "            f.write(json_dict)\n",
    "        print(\"\\nModel Saved.\")\n",
    "save_model(m)\n",
    "</code>\n",
    "\n",
    "At last line of above code, we are calling a method to save our model. If we looked to our local directory, then there is a json file.\n",
    "\n",
    "## 5.2 Load Model\n",
    "This method can be treat as independent method.\n",
    "\n",
    "<code>\n",
    "def load_model(path=\"model.json\"):\n",
    "    \"\"\"\n",
    "        path:- path of model file including filename        \n",
    "        returns:- a model\n",
    "    \"\"\"    \n",
    "    models = {\"CNN\": CNN}\n",
    "    layers = {\"FFL\": FFL, \"Conv2d\":Conv2d, \"Dropout\":Dropout, \"Flatten\": Flatten, \"Pool2d\":Pool2d}\n",
    "    with open(path, \"r\") as f:\n",
    "        dict_model = json.load(f)\n",
    "        model = dict_model[\"model\"]\n",
    "        model = models[model]()\n",
    "        for layer, params in dict_model.items():\n",
    "            if layer != \"model\":\n",
    "                lyr_type = layers[params[\"type\"]]               \n",
    "                if lyr_type == FFL:                                        \n",
    "                    lyr.neurons = params[\"neurons\"]\n",
    "                    lyr = layers[params[\"type\"]](neurons=params[\"neurons\"])                \n",
    "                if lyr_type == Conv2d:\n",
    "                    lyr = layers[params[\"type\"]](filters=int(params[\"filters\"]), kernel_size=params[\"kernel_size\"], padding=params[\"padding\"])\n",
    "                    lyr.out = np.zeros(params[\"output_shape\"])\n",
    "                    params[\"input_shape\"] = tuple(params[\"input_shape\"])\n",
    "                    params[\"output_shape\"] = tuple(params[\"output_shape\"])\n",
    "                if lyr_type == Dropout:\n",
    "                    lyr = layers[params[\"type\"]](prob=params[\"prob\"])\n",
    "                    try:\n",
    "                        params[\"input_shape\"] = tuple(params[\"input_shape\"])\n",
    "                        params[\"output_shape\"] = tuple(params[\"output_shape\"])\n",
    "                    except:\n",
    "                        pass                    \n",
    "                if lyr_type == Pool2d:\n",
    "                    lyr = layers[params[\"type\"]](kernel_size = params[\"kernel_size\"], stride=params[\"stride\"], kind=params[\"kind\"])\n",
    "                    params[\"input_shape\"] = tuple(params[\"input_shape\"])\n",
    "                    try:\n",
    "                        params[\"output_shape\"] = tuple(params[\"output_shape\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                if lyr_type == Flatten:\n",
    "                    params[\"input_shape\"] = tuple(params[\"input_shape\"])                    \n",
    "                    lyr = layers[params[\"type\"]](input_shape=params[\"input_shape\"])\n",
    "                lyr.name = layer\n",
    "                lyr.activation = params[\"activation\"]\n",
    "                lyr.isbias = params[\"isbias\"]\n",
    "                lyr.input_shape = params[\"input_shape\"]\n",
    "                lyr.output_shape = params[\"output_shape\"]\n",
    "                lyr.parameters = int(params[\"parameters\"])                \n",
    "                if params.get(\"weights\"):\n",
    "                    lyr.weights = np.array(params[\"weights\"])                \n",
    "                if params.get(\"biases\"):\n",
    "                    lyr.biases = np.array(params[\"biases\"])                               \n",
    "                model.layers.append(lyr)\n",
    "        print(\"Model Loaded...\")\n",
    "        return model\n",
    "mm = load_model()\n",
    "mm.summary()\n",
    "m.predict(x[0]) == mm.predict(x[0])\n",
    "</code>\n",
    "\n",
    "On above block of code, we tried to load a model. I am not going to describe much here but we are printing summary and then checking if the prediction from original model and loaded model is right or wrong. If our model is loaded properly, then the array of all `True` will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample Layer\n",
    "Note that, `Pooling Layer` can be called as downsampling layer because it takes samples of pixels and returns new image with shape lesser than original image. And the opposite of this layer is `Upsample Layer`. Upsample layer generally increase the size of shape, in more simple words, it zooms the image. And if we see to the configuration of `YOLO(You Only Look Once)` authors have used multiple times `Upsample Layer`. For simpler case, i am doing the pixels expansion. \n",
    "Lets take an example(on my case):\n",
    "\n",
    "\\begin{pmatrix}\n",
    "12 & 10 \\\\ \n",
    "101 & 88 \\end{pmatrix}\n",
    "\n",
    "The output after the kernel (2, 2) will be<i>(the kernel here will not exactly the kernel like on Maxpool or CNN but it will be used as expansion rate of (row, col))</i>:-\n",
    "\n",
    "\\begin{pmatrix}\n",
    "12 & 12 & 10 & 10\\\\ \n",
    "12 & 12 & 10 & 10\\\\\n",
    "101 & 101 & 88 & 88\\\\ \n",
    "101 & 101 & 88 & 88\\end{pmatrix}\n",
    "\n",
    "This is just a simple case of Upsampling, and i haven not done much research about it.\n",
    "\n",
    "<code>\n",
    "class Upsample:\n",
    "    def __init__(self, kernel_size = (2, 2)):\n",
    "        self.input_shape=None\n",
    "        self.output_shape = None\n",
    "        self.input_data= None\n",
    "        self.output = None\n",
    "        self.isbias = False\n",
    "        self.activation = None\n",
    "        self.parameters = 0\n",
    "        self.delta = 0\n",
    "        self.weights = 0\n",
    "        self.bias = 0\n",
    "        self.delta_weights = 0\n",
    "        self.delta_biases = 0        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = self.kernel_size\n",
    "    def set_output_shape(self):\n",
    "        shape = self.input_shape\n",
    "        self.output_shape = (shape[0] * self.kernel_size[0], shape[1] * self.kernel_size[1], shape[2])\n",
    "        self.weights = 0\n",
    "    def apply_activation(self, image):\n",
    "        stride = self.stride\n",
    "        kshape = self.kernel_size\n",
    "        self.input_shape = image.shape\n",
    "        self.set_output_shape()\n",
    "        rstep = stride[0]\n",
    "        cstep = stride[1]\n",
    "        self.out = np.zeros(self.output_shape)\n",
    "        shape = self.output_shape\n",
    "        for nc in range(shape[2]):\n",
    "            cimg = []\n",
    "            rv = 0\n",
    "            i = 0\n",
    "            for r in range(kshape[0], shape[0]+1, rstep):\n",
    "                cv = 0\n",
    "                j = 0\n",
    "                for c in range(kshape[1], shape[1]+1, cstep):\n",
    "                    self.out[rv:r, cv:c] = image[i, j]\n",
    "                    j+=1\n",
    "                    cv+=cstep\n",
    "                rv+=rstep\n",
    "                i+=1                \n",
    "        return self.out\n",
    "    def backpropagate(self, nx_layer):\n",
    "        \"\"\"\n",
    "            Gradients are passed through index of largest value .\n",
    "        \"\"\"\n",
    "        layer = self\n",
    "        stride = layer.stride\n",
    "        kshape = layer.kernel_size\n",
    "        image = layer.input\n",
    "        shape = image.shape\n",
    "        layer.delta = np.zeros(shape)        \n",
    "        cimg = []\n",
    "        rstep = stride[0]\n",
    "        cstep = stride[1]\n",
    "        shape = nx_layer.delta.shape\n",
    "        for f in range(shape[2]):\n",
    "            i = 0\n",
    "            rv = 0\n",
    "            for r in range(kshape[0], shape[0]+1, rstep):\n",
    "                cv = 0\n",
    "                j = 0\n",
    "                for c in range(kshape[1], shape[1]+1, cstep):\n",
    "                    dout = nx_layer.delta[rv:r, cv:c, f]\n",
    "                    layer.delta[i, j, f] = dout\n",
    "                    j+=1\n",
    "                    cv+=cstep\n",
    "                rv+=rstep\n",
    "                i+=1            \n",
    "</code>\n",
    "\n",
    "I edited the code of `Pool2d` for this and `backpropagate` is bit different. You can test this code by:-\n",
    "\n",
    "<code>\n",
    "us = Upsample(kernel_size=(1, 3))\n",
    "img = us.apply_activation(x_train[0].reshape(28, 28, 1))\n",
    "plt.imshow(img.reshape(28, 28*3))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Learned Features\n",
    "Well, we trained a model but what actually did a model learned? We will be taking the model that we saved earlier. It is loaded on `mm`. And now we will loop through all layers and corresponding weights are visualized.\n",
    "\n",
    "<code>\n",
    "for l in mm.layers:\n",
    "    if type(l).__name__ == \"Conv2d\":\n",
    "        for f in range(l.filters):\n",
    "            for c in range(l.weights.shape[2]):\n",
    "                plt.imshow(l.weights[:, :, c, f])\n",
    "                plt.title(f\"Layer: {l.name} Filter: {f} Channel: {c}\")\n",
    "                plt.show()\n",
    "    if type(l).__name__ == \"FFL\":\n",
    "        plt.imshow(l.weights)\n",
    "        plt.title(l.name)\n",
    "        plt.show()\n",
    "</code>\n",
    "\n",
    "### More on Visualization\n",
    "How will an test image change through the layers? Lets try to find out. When a image gets into any CNN layer, we apply the filters to each channel and sum them. Our `feedforward` method has granted us huge application because we can set the `input` and `output` of each layer for current example. And yes thats what we are using.\n",
    "\n",
    "<code>\n",
    "timg = x[0]\n",
    "op = mm.predict(x[0])\n",
    "for l in mm.layers:\n",
    "    print(l.name)\n",
    "    if type(l).__name__ == \"Conv2d\":\n",
    "        fig = plt.figure(figsize=(30, 30))\n",
    "        cols = l.filters * 2\n",
    "        rows = 1\n",
    "        f = 0\n",
    "        for i in range(0, cols*rows):\n",
    "            fig.add_subplot(rows, cols, i+1)                    \n",
    "            if i % 2 == 0:\n",
    "                if f < l.filters:\n",
    "                    plt.imshow(l.out[:, :, f], cmap=\"gray\")                   \n",
    "            else:\n",
    "                if f < l.filters:\n",
    "                    cimg = l.weights[:, :, 0, f]\n",
    "                    plt.imshow(cimg, cmap='gray')\n",
    "                    plt.title(f\"Layer: {l.name} Filter: {f}\")\n",
    "                    f+=1                            \n",
    "    if type(l).__name__ == \"Pool2d\":\n",
    "        fig = plt.figure(figsize=(30, 30))\n",
    "        cols = l.out.shape[2] * 2\n",
    "        rows = 1\n",
    "        print(\"Input\\n\")\n",
    "        for f in range(l.out.shape[2]):\n",
    "            fig.add_subplot(rows, cols, f+1)                    \n",
    "            plt.imshow(l.input[:, :, f], cmap=\"gray\")\n",
    "            plt.title(f\"Layer: {l.name} Filter: {f}\")\n",
    "        plt.show()\n",
    "        fig = plt.figure(figsize=(30, 30))\n",
    "        print(\"Output\\n\")\n",
    "        for f in range(l.out.shape[2]):\n",
    "            fig.add_subplot(rows, cols, f+1)                    \n",
    "            plt.imshow(l.out[:, :, f], cmap=\"gray\")\n",
    "            plt.title(f\"Layer: {l.name} Filter: {f}\")\n",
    "    if type(l).__name__ == \"Dropout\":\n",
    "        try:\n",
    "            fig = plt.figure(figsize=(30, 30))\n",
    "            cols = l.out.shape[2] * 2\n",
    "            rows = 1\n",
    "            print(\"Input\\n\")\n",
    "            for f in range(l.out.shape[2]):\n",
    "                fig.add_subplot(rows, cols, f+1)                    \n",
    "                plt.imshow(l.input[:, :, f], cmap=\"gray\")\n",
    "                plt.title(f\"Layer: {l.name} Filter: {f}\")\n",
    "            plt.show()\n",
    "            fig = plt.figure(figsize=(30, 30))\n",
    "            print(\"Output\\n\")\n",
    "            for f in range(l.out.shape[2]):\n",
    "                fig.add_subplot(rows, cols, f+1)                    \n",
    "                plt.imshow(l.out[:, :, f], cmap=\"gray\")\n",
    "                plt.title(f\"Layer: {l.name} Filter: {f}\")\n",
    "        except:\n",
    "            pass\n",
    "    plt.show()\n",
    "</code>\n",
    "\n",
    "# image here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     324,
     356,
     394,
     397,
     414
    ]
   },
   "source": [
    "# 6 References:\n",
    "I have not done all these codes by myself. I have tried to give credits and references whenever i borrowed concepts and codes. I got help from googling and mostly stackoverflow. However i have to mentions some of great resources at last:-\n",
    "* [Optimizers code were referenced from here](https://www.github.com/ShivamShrirao/dnn_from_scratch)\n",
    "* [An Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/index.html)\n",
    "* [Convolutional Neural Network from Ground Up](https://towardsdatascience.com/convolutional-neural-network-from-ground-up-c67bb41454e1)\n",
    "* [A Gentle Introduction to CNN](https://sefiks.com/2017/11/03/a-gentle-introduction-to-convolutional-neural-networks/)\n",
    "* [Training a Convolutional Neural Network](https://victorzhou.com/blog/intro-to-cnns-part-2/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     324,
     356,
     394,
     397,
     414
    ]
   },
   "source": [
    "# 7 You might like to view:-\n",
    "* [Writing Popular Machine Learning Optimizers from Scratch on Python](https://acharyaramkrishna.com.np/2020/06/05/writing-popular-machine-learning-optimizers-from-scratch-on-python/)\n",
    "* [Writing Image Processing Class From Scratch on Python](https://acharyaramkrishna.com.np/2020/05/31/image-processing-class-from-scratch-on-python/)\n",
    "* [Writing a Deep Neural Network from Scratch on Python](https://acharyaramkrishna.com.np/2020/05/31/writing-a-deep-neural-network-from-scratch-on-python/)\n",
    "* [Convolutional Neural Networks from Scratch on Python](https://acharyaramkrishna.com.np/2020/06/05/convolutional-neural-networks-from-scratch-on-python/)\n",
    "\n",
    "For the production phase, it is always best idea to use frameworks but for the learning phase, starting from the scratch is a great idea. I also got suggestions from friends that, prof. Adrew Ng's contents drives us through the scratch but i never got chance to watch one. I am sharing a notebook and repository link also. On next blog i will try to do <strong>RNN</strong> from scratch. Please leave a feedback, and if you find this good content then sharing is caring. Thank you for your time and please ping me on <b>[twitter](https://twitter.com/Quassarianviper)</b>. You can find all these files under [ML From Basics](https://github.com/q-viper/ML-from-Basics).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
